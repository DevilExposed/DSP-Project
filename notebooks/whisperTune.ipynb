{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efb6fd8",
   "metadata": {},
   "source": [
    "This notebook is designed to be run in Google Colab, which provides a pre-configured Python environment with many common data science and machine learning libraries already installed.\n",
    "Package Installation Syntax:\n",
    "In Colab, you can install additional Python packages using the ! at the start of a cell:\n",
    "This is different from Jupyter notebooks run locally, where you might use %pip install or run pip in a terminal.\n",
    "\n",
    "Some imports may not have explicit installation commands in the notebook because they are already available in the Colab environment by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies\n",
    "!pip -q install transformers datasets jiwer accelerate sentencepiece ipywidgets soundfile librosa torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for data processing and model training\n",
    "import gc, random, torch, inspect\n",
    "from itertools import islice\n",
    "from random import randint\n",
    "\n",
    "# Importing Hugging Face datasets and model components\n",
    "from datasets import load_dataset, load_dataset_builder, Dataset, interleave_datasets, Audio, Features, Value\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from huggingface_hub import login\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Authenticating with Hugging Face using API token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ae72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining supported languages and priority languages for training\n",
    "languages = [\"en\", \"bg\", \"uk\", \"ru\", \"ar\", \"it\", \"pl\", \"pt\"]\n",
    "priority_languages = [\"bg\", \"uk\", \"ru\", \"ar\", \"pl\"]\n",
    "CAP_HR, CAP_LR = 20_000, 50_000\n",
    "\n",
    "# Defining audio feature specifications\n",
    "AUDIO_FT = Audio(sampling_rate=16_000)  # Setting audio sampling rate to 16kHz\n",
    "FEATS = Features({\n",
    "    \"audio\": AUDIO_FT,        # Audio feature configuration\n",
    "    \"sentence\": Value(\"string\")  # Text transcription feature\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12aae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_to_ds(lang: str, split: str, cap: int) -> Dataset:\n",
    "    \"\"\"\n",
    "    Creates a streaming dataset for a specific language and split.\n",
    "    \n",
    "    Args:\n",
    "        lang (str): Language code\n",
    "        split (str): Dataset split \n",
    "        cap (int): Maximum number of samples to include\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Hugging Face dataset with audio and transcriptions\n",
    "    \"\"\"\n",
    "    # Loading the Common Voice dataset for the specified language\n",
    "    stream = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        lang, \n",
    "        split=split, \n",
    "        streaming=True,  # Enable streaming to handle large datasets\n",
    "        trust_remote_code=True\n",
    "    ).shuffle(buffer_size=10_000, seed=42)  # Shuffle with fixed seed for reproducibility\n",
    "\n",
    "    def generator():\n",
    "        # Extracting audio samples and their transcriptions up to the specified cap\n",
    "        for ex, _ in zip(stream, range(cap)):\n",
    "            yield {\"audio\": ex[\"audio\"], \"sentence\": ex[\"sentence\"]}\n",
    "\n",
    "    return Dataset.from_generator(generator, features=FEATS)\n",
    "\n",
    "def split_available(lang: str, split: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a specific split is available for a language in Common Voice.\n",
    "    \n",
    "    Args:\n",
    "        lang (str): Language code\n",
    "        split (str): Dataset split to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if split exists and has samples, False otherwise\n",
    "    \"\"\"\n",
    "    info = load_dataset_builder(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        lang, \n",
    "        trust_remote_code=True\n",
    "    ).info\n",
    "    \n",
    "    return bool(info.splits) and split in info.splits and info.splits[split].num_examples > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d10e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building multi-language dataset\n",
    "print(\"Building multi-language dataset...\")\n",
    "small_sets = []\n",
    "EXTRA_TRAIN = 20_000  # Additional training samples for languages with limited data\n",
    "\n",
    "for lang in languages:\n",
    "    if lang in priority_languages:\n",
    "        # For priority languages, use validation set and either 'other' split or extra training data\n",
    "        val = stream_to_ds(lang, \"validation\", CAP_LR)\n",
    "\n",
    "        parts = [val]\n",
    "        if split_available(lang, \"other\"):\n",
    "            # Use 'other' split if available\n",
    "            parts.append(stream_to_ds(lang, \"other\", CAP_LR))\n",
    "        else:\n",
    "            # Fallback: use a small portion of training data\n",
    "            parts.append(stream_to_ds(lang, \"train\", EXTRA_TRAIN))\n",
    "\n",
    "        ds = interleave_datasets(parts)\n",
    "        small_sets.append(ds)\n",
    "    else:\n",
    "        # For non-priority languages (EN, IT, PT), use validation set with higher cap\n",
    "        val = stream_to_ds(lang, \"validation\", CAP_HR)\n",
    "        small_sets.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6167b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise augmentation setup\n",
    "def get_noise_sampler():\n",
    "    \"\"\"\n",
    "    Creates a noise sampler that provides background noise for data augmentation.\n",
    "    Attempts to use CAIMAN noise dataset, falls back to UrbanSound8K if unavailable.\n",
    "    \n",
    "    Returns:\n",
    "        function: A function that returns a random noise sample\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try loading CAIMAN background noise dataset\n",
    "        ns = load_dataset(\n",
    "            \"Myrtle/CAIMAN-ASR-BackgroundNoise\",\n",
    "            split=\"train\", \n",
    "            streaming=True\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"CAIMAN noise missing -> UrbanSound8K fallback\")\n",
    "        # Fallback to UrbanSound8K dataset\n",
    "        ns = load_dataset(\n",
    "            \"danavery/urbansound8K\",\n",
    "            split=\"train\", \n",
    "            streaming=True\n",
    "        )\n",
    "    # Create a bank of 200 noise samples\n",
    "    bank = [row[\"audio\"][\"array\"] for row in islice(ns, 200)]\n",
    "    def _sample(): return random.choice(bank)\n",
    "    return _sample\n",
    "\n",
    "# Initialize noise sampler\n",
    "get_noise = get_noise_sampler()\n",
    "\n",
    "def match_length(sig: torch.Tensor, tgt_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Adjusts the length of an audio signal to match a target length.\n",
    "    \n",
    "    Args:\n",
    "        sig (torch.Tensor): Input audio signal\n",
    "        tgt_len (int): Target length in samples\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Adjusted audio signal matching target length\n",
    "    \"\"\"\n",
    "    cur = sig.shape[-1]\n",
    "    if cur == tgt_len:\n",
    "        return sig\n",
    "    if cur < tgt_len:\n",
    "        # If signal is too short, repeat it\n",
    "        reps = tgt_len // cur + 1\n",
    "        return sig.repeat(reps)[:tgt_len]\n",
    "    # If signal is too long, truncate it\n",
    "    return sig[:tgt_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e024a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model and processor\n",
    "HF_MODEL = \"openai/whisper-base\"\n",
    "processor = WhisperProcessor.from_pretrained(HF_MODEL)\n",
    "\n",
    "\n",
    "def whisper_data_collator(batch, snr_db: int = 15):\n",
    "    \"\"\"\n",
    "    Collates and preprocesses batches of data for Whisper model training.\n",
    "    Includes noise augmentation for improved robustness.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of examples containing audio and text\n",
    "        snr_db (int): Signal-to-noise ratio in decibels for noise augmentation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed batch with features and labels\n",
    "    \"\"\"\n",
    "    feats, ids = [], []\n",
    "    for ex in batch:\n",
    "        # Convert audio to tensor and ensure float32 type\n",
    "        wav = torch.tensor(ex[\"audio\"][\"array\"]).float()\n",
    "\n",
    "        # Apply noise augmentation to 10% of samples\n",
    "        if random.random() < 0.1:\n",
    "            noise = torch.tensor(get_noise()).float()\n",
    "            # Handle mono/stereo conversion\n",
    "            if noise.dim() == 2: noise = noise[0]\n",
    "            if wav.dim() == 2: wav = wav[0]\n",
    "            # Match noise length to audio length\n",
    "            noise = match_length(noise, wav.shape[-1])\n",
    "            # Add scaled noise based on SNR\n",
    "            wav += wav.std() / (10**(snr_db/20) * noise.std() + 1e-9) * noise\n",
    "\n",
    "        # Extract features and tokenize text\n",
    "        feats.append(\n",
    "            processor.feature_extractor(\n",
    "                wav.numpy(), sampling_rate=16_000, return_tensors=\"pt\"\n",
    "            ).input_features[0]\n",
    "        )\n",
    "        ids.append(\n",
    "            processor.tokenizer(\n",
    "                ex[\"sentence\"], return_tensors=\"pt\"\n",
    "            ).input_ids[0]\n",
    "        )\n",
    "\n",
    "    # Pad features and labels to same length within batch\n",
    "    batch_in = processor.feature_extractor.pad(\n",
    "        {\"input_features\": feats}, return_tensors=\"pt\"\n",
    "    )\n",
    "    batch_lab = processor.tokenizer.pad(\n",
    "        {\"input_ids\": ids}, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    # Replace padding tokens with -100 for loss calculation\n",
    "    batch_lab[\"input_ids\"][batch_lab[\"attention_mask\"] == 0] = -100\n",
    "    batch_in[\"labels\"] = batch_lab[\"input_ids\"]\n",
    "    return batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0278c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(HF_MODEL)\n",
    "model.freeze_encoder() # Freeze encoder weights\n",
    "    \n",
    "# Unfreezing last 12 encoder layers for fine-tuning\n",
    "for p in model.model.encoder.layers[-12:].parameters():\n",
    "    p.requires_grad = True\n",
    "model.gradient_checkpointing_enable() # Enable gradient checkpointing for memory efficiency\n",
    "\n",
    "# Reset generation-related config for multilingual support\n",
    "for cfg in (model.config, model.generation_config):\n",
    "    cfg.forced_decoder_ids = None\n",
    "    cfg.suppress_tokens = None\n",
    "    cfg.begin_suppress_tokens = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30866f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "base_kwargs = dict(\n",
    "    output_dir = \"/content/whisper-ft\", # Directory to save model checkpoints\n",
    "    per_device_train_batch_size = 4, # Batch size per GPU/CPU\n",
    "    gradient_accumulation_steps = 8, # Accumulate gradients over multiple steps\n",
    "    num_train_epochs = 8, # Total number of training epochs\n",
    "    learning_rate = 1e-5, # Initial learning rate\n",
    "    warmup_steps = 500, # Number of warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type = \"cosine\", # Learning rate scheduler type\n",
    "    eval_steps = 1000, # Evaluate every N steps\n",
    "    save_steps = 1000, # Save checkpoint every N steps\n",
    "    save_total_limit = 1, # Keep only N most recent checkpoints\n",
    "    logging_steps = 50, # Log training metrics every N steps\n",
    "    fp16 = torch.cuda.is_available(), # Use mixed precision training if CUDA available\n",
    "    remove_unused_columns = False, # Keep all columns in dataset\n",
    ")\n",
    "\n",
    "# Filter training arguments based on available parameters\n",
    "sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    **{k: v for k, v in base_kwargs.items() if k in sig.parameters}\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model, # Fine-tuned Whisper model\n",
    "    args = args, # Training arguments\n",
    "    train_dataset = train_ds, # Training dataset\n",
    "    eval_dataset = eval_ds, # Evaluation dataset\n",
    "    data_collator = whisper_data_collator, # Custom data collator with augmentation\n",
    "    tokenizer = processor.tokenizer, # Tokenizer for text processing\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32942a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"/content/whisper-ft\")\n",
    "model.save_pretrained(\"/content/whisper-ft\")\n",
    "processor.save_pretrained(\"/content/whisper-ft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "# Download the fine-tuned model\n",
    "shutil.make_archive(\"whisper-ft\", 'zip', \"/content/whisper-ft\")\n",
    "files.download(\"whisper-ft.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
